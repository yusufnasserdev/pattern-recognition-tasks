import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import PolynomialFeatures
from sklearn import metrics
import seaborn as sns


import warnings
warnings.filterwarnings("ignore")

# Loading data
df = pd.read_csv('SuperMarketSales.csv')

def date_to_float(dt):
    """Reduces the date to a float

    Args:
        dt (datetime): The date to be reduced
        
    Returns:
        float: The reduced date
    """
    
    # Calculating the months and days
    calc = (((dt.month - 1) * 30) + dt.day) / 365
    # Adding calc to the years
    return dt.year + calc

# Parsing the date
df['Date'] = pd.to_datetime(df['Date'], dayfirst=True)

# Extracting the month and the day
df['Day'] = df['Date'].dt.day
df['Month'] = df['Date'].dt.month

# Reducing the date to a float
df['Date'] = df['Date'].apply(date_to_float)   

# Dropping the rows with missing values
df = df.dropna()

# Getting the correlation matrix
corr = df.corr()

## All the features are in about the same correlation range with the target variable

# Eliminate weekly sales column outliers
df = df[(np.abs(stats.zscore(df['Weekly_Sales'])) < 3)]

# Extracting the target variable column and dropping it from the dataset
Y = df['Weekly_Sales']
df = df.drop(['Weekly_Sales'], axis=1)

# Data splitting
X_train, X_test, Y_train, Y_test = train_test_split(df, Y, test_size=0.2, random_state=42)

dg = 2

def poly_features(X, degree):
    """Generates polynomial features for the given features

    Note: The polynomial features generated are more than the ones generated by the PolynomialFeatures class
    since it generates the polynomial features for all the feature combinations, it doesn't take into consideration
    that some features are the same.
    
    Args:
        X (numpy.ndarray): The features
        degree (int): The degree of the polynomial features

    Returns:
        numpy.ndarray: The polynomial features
    """
    X = X.values if isinstance(X, pd.DataFrame) else X
    # Create an empty array for the polynomial features
    X_poly = np.empty((len(X), 0))
    
    # Generate the polynomial features for each feature combination
    for i in range(X.shape[1]):
        for j in range(i, X.shape[1]):
            # Get the i-th and j-th features
            feature_i = X[:, i]
            feature_j = X[:, j]
            
            # Generate the polynomial features for all the i-th and j-th features combinations
            # from degree 1 to the given degree and append them to the array
            for u in range(1, degree+1):
                for v in range(u+1):
                    feature_ij_poly = (feature_i**(u-v)) * (feature_j**v)
                    X_poly = np.hstack((X_poly, feature_ij_poly.reshape(-1, 1)))
    
    return X_poly

# Generate the polynomial features
X_train_poly = poly_features(X_train, dg)
X_test_poly = poly_features(X_test, dg)

# Train the model
model = LinearRegression()
model.fit(X_train_poly, Y_train)

mse = metrics.mean_squared_error(Y_test, model.predict(X_test_poly))
# Print the mse and r2 score
print('MSE:', mse)
print('R2:', metrics.r2_score(Y_test, model.predict(X_test_poly)))

while mse > 5 and dg < 6:
    dg += 1
    # Generate the polynomial features
    X_train_poly = poly_features(X_train, dg)
    X_test_poly = poly_features(X_test, dg)

    # Train the model
    model = LinearRegression()
    model.fit(X_train_poly, Y_train)

    # Print the mse and r2 score
    mse = metrics.mean_squared_error(Y_test, model.predict(X_test_poly))
    print('MSE:', mse)
    print('R2:', metrics.r2_score(Y_test, model.predict(X_test_poly)))

# Best degrees are:
#      2 with MSE: 216926470962.73593 and R2: 0.30109798522248654
#      3 with MSE: 249317366737.26962 and R2: 0.19673975629449714
# Degrees more than 3 are overfitting with the R2 score is edging away from 1 
# indicating that the model variance is growing and the model is not generalizing well